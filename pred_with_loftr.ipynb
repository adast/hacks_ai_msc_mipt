{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet50\n",
    "\n",
    "# Others\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet50()\n",
    "model.fc = nn.Linear(2048, 4)\n",
    "\n",
    "checkpoint = torch.load('/home/vdd/MIPT/v2/checkpoints/epoch-205_loss_0.00018.pt', map_location='cpu')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SATDataset(Dataset):\n",
    "    def __init__(self, paths):\n",
    "        self.paths = paths\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        img_path = self.paths[index]\n",
    "        roi = cv2.imread(img_path)\n",
    "        roi = cv2.cvtColor(roi, cv2.COLOR_BGR2RGB)\n",
    "        roi = self.to_tensor(roi)\n",
    "        roi = self.normalize(roi)\n",
    "        return roi\n",
    "\n",
    "img_paths = sorted(glob.glob('/home/vdd/main/vdd/sat_model/test/*.png'), key=lambda x: int(x.split('/')[-1].split('.')[0]))\n",
    "\n",
    "BATCH_SIZE = 2\n",
    "NUM_WORKERS = 4\n",
    "dataset = SATDataset(img_paths)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bfc1638f02a4a2c97c52b495abeeec9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "centers = []\n",
    "angles = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dataloader):\n",
    "        batch = batch.to(device)\n",
    "        result = model.forward(batch)\n",
    "        result = result.detach().cpu().numpy()\n",
    "        centers.append(result[:, :2] * 10496)\n",
    "        angles.append(np.round(np.rad2deg(np.arctan2(result[:, 2], result[:, 3])) % 360).astype(int))\n",
    "\n",
    "centers = np.concatenate(centers)\n",
    "angles = np.concatenate(angles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kornia as K\n",
    "import kornia.feature as KF\n",
    "from kornia_moons.feature import *\n",
    "\n",
    "matcher = KF.LoFTR(pretrained='outdoor').cuda().eval()\n",
    "\n",
    "def find_rigid_alignment(A, B):\n",
    "    \"\"\"\n",
    "    See: https://en.wikipedia.org/wiki/Kabsch_algorithm\n",
    "    2-D or 3-D registration with known correspondences.\n",
    "    Registration occurs in the zero centered coordinate system, and then\n",
    "    must be transported back.\n",
    "        Args:\n",
    "        -    A: Torch tensor of shape (N,D) -- Point Cloud to Align (source)\n",
    "        -    B: Torch tensor of shape (N,D) -- Reference Point Cloud (target)\n",
    "        Returns:\n",
    "        -    R: optimal rotation\n",
    "        -    t: optimal translation\n",
    "    Test on rotation + translation and on rotation + translation + reflection\n",
    "        >>> A = torch.tensor([[1., 1.], [2., 2.], [1.5, 3.]], dtype=torch.float)\n",
    "        >>> R0 = torch.tensor([[np.cos(60), -np.sin(60)], [np.sin(60), np.cos(60)]], dtype=torch.float)\n",
    "        >>> B = (R0.mm(A.T)).T\n",
    "        >>> t0 = torch.tensor([3., 3.])\n",
    "        >>> B += t0\n",
    "        >>> R, t = find_rigid_alignment(A, B)\n",
    "        >>> A_aligned = (R.mm(A.T)).T + t\n",
    "        >>> rmsd = torch.sqrt(((A_aligned - B)**2).sum(axis=1).mean())\n",
    "        >>> rmsd\n",
    "        tensor(3.7064e-07)\n",
    "        >>> B *= torch.tensor([-1., 1.])\n",
    "        >>> R, t = find_rigid_alignment(A, B)\n",
    "        >>> A_aligned = (R.mm(A.T)).T + t\n",
    "        >>> rmsd = torch.sqrt(((A_aligned - B)**2).sum(axis=1).mean())\n",
    "        >>> rmsd\n",
    "        tensor(3.7064e-07)\n",
    "    \"\"\"\n",
    "    a_mean = A.mean(axis=0)\n",
    "    b_mean = B.mean(axis=0)\n",
    "    A_c = A - a_mean\n",
    "    B_c = B - b_mean\n",
    "    # Covariance matrix\n",
    "    H = A_c.T.mm(B_c)\n",
    "    U, S, V = torch.svd(H)\n",
    "    # Rotation matrix\n",
    "    R = V.mm(U.T)\n",
    "    # Translation vector\n",
    "    t = b_mean[None, :] - R.mm(a_mean[None, :].T).T\n",
    "    t = t.T\n",
    "    return R, t.squeeze()\n",
    "\n",
    "def extract_region(img, center, size, angle):\n",
    "    # Extract region from image around the center\n",
    "    radius = np.ceil(np.sqrt(size** 2 * 2) / 2).astype(int)\n",
    "    assert min(center) >= radius, 'center is too close to the border'\n",
    "    cx, cy = center\n",
    "    roi = img[cy-radius:cy+radius, cx-radius:cx+radius]\n",
    "\n",
    "    # Rotate this region\n",
    "    h, w = roi.shape[:2]\n",
    "    M = cv2.getRotationMatrix2D((w // 2, h // 2), angle, 1.0)\n",
    "    roi = cv2.warpAffine(roi, M, (w, h))\n",
    "\n",
    "    # Center crop roi\n",
    "    start_y = (h - size) // 2\n",
    "    start_x = (w - size) // 2\n",
    "    roi = roi[start_y:start_y+size, start_x:start_x+size]\n",
    "\n",
    "    return roi\n",
    "\n",
    "sat_map = cv2.imread('/home/vdd/main/vdd/sat_model/original.tiff', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65c173546bd74e61b8b5b34534788363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vdd/envs/torch_vdd/lib/python3.10/site-packages/kornia/feature/loftr/utils/coarse_matching.py:243: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  mkpts0_c = torch.stack([i_ids % data['hw0_c'][1], i_ids // data['hw0_c'][1]], dim=1) * scale0\n",
      "/home/vdd/envs/torch_vdd/lib/python3.10/site-packages/kornia/feature/loftr/utils/coarse_matching.py:244: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  mkpts1_c = torch.stack([j_ids % data['hw1_c'][1], j_ids // data['hw1_c'][1]], dim=1) * scale1\n"
     ]
    }
   ],
   "source": [
    "delta_rots = []\n",
    "delta_trans = []\n",
    "for center, angle, tgt_img_path in tqdm(zip(centers, angles, img_paths)):\n",
    "    delta_angle = 0\n",
    "    translation = np.array([0, 0])\n",
    "    if (center > 726).all() & (center < 9770).all():\n",
    "        try:\n",
    "            sat_img = extract_region(sat_map, np.round(center).astype(int), 1024, angle)\n",
    "            sat_img = transforms.ToTensor()(sat_img).to(device).unsqueeze(dim=0)\n",
    "            tgt_img = cv2.imread(tgt_img_path, 0)\n",
    "            tgt_img = transforms.ToTensor()(tgt_img).to(device).unsqueeze(dim=0)\n",
    "\n",
    "            input_dict = {\"image0\": sat_img, \"image1\": tgt_img}\n",
    "            with torch.no_grad():\n",
    "                correspondences = matcher(input_dict)\n",
    "\n",
    "            mkpts0 = correspondences['keypoints0'].cpu().numpy()\n",
    "            mkpts1 = correspondences['keypoints1'].cpu().numpy()\n",
    "\n",
    "            H, inliers = cv2.findFundamentalMat(mkpts0, mkpts1, cv2.USAC_MAGSAC, 0.5, 0.999, 5000)\n",
    "            inliers = inliers > 0\n",
    "            \n",
    "            if sum(inliers) > 200:\n",
    "                R, t = find_rigid_alignment(correspondences['keypoints0'][inliers.squeeze()], correspondences['keypoints1'][inliers.squeeze()])\n",
    "                delta_angle = torch.rad2deg(torch.atan2(R[0][1], R[0][0])).detach().cpu().item()\n",
    "                translation = t.detach().cpu().numpy()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    delta_rots.append(delta_angle)\n",
    "    delta_trans.append(translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate(p, origin=(0, 0), degrees=0):\n",
    "    angle = np.deg2rad(degrees)\n",
    "    R = np.array([[np.cos(angle), -np.sin(angle)],\n",
    "                  [np.sin(angle),  np.cos(angle)]])\n",
    "    o = np.atleast_2d(origin)\n",
    "    p = np.atleast_2d(p)\n",
    "    return np.squeeze((R @ (p.T-o.T) + o.T).T)\n",
    "\n",
    "results = []\n",
    "HALF_SIZE = 512\n",
    "for center, angle, trans, rot in zip(centers, angles, delta_trans, delta_rots):\n",
    "    bbox = np.array([\n",
    "        center + np.array([-HALF_SIZE, -HALF_SIZE]) - trans,\n",
    "        center + np.array([HALF_SIZE, -HALF_SIZE]) - trans,\n",
    "        center + np.array([-HALF_SIZE, HALF_SIZE]) - trans,\n",
    "        center + np.array([HALF_SIZE, HALF_SIZE]) - trans,\n",
    "    ])\n",
    "    bbox = rotate(bbox, center, angle)\n",
    "    bbox = np.round(bbox).astype(int)\n",
    "    results.append({\n",
    "        'left_top': bbox[0].tolist(),\n",
    "        'right_top': bbox[1].tolist(),\n",
    "        'left_bottom': bbox[2].tolist(),\n",
    "        'right_bottom': bbox[3].tolist(),\n",
    "        'angle': np.round(angle.item() - rot).astype(int).item()\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "for pred, path in zip(results, img_paths):\n",
    "    name = path.split('/')[-1].split('.')[0]\n",
    "    with open(f'pred/{name}.json', 'w') as f:\n",
    "        json.dump(pred, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('torch_vdd')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "725e838e178c99cd5b2db7d2a591b336282772958be54977f5c1a9d7bb163b25"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
